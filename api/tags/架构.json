{"name":"架构","slug":"架构","count":4,"posts":[{"title":"拜托！面试请不要再问我Spring Cloud底层原理","slug":"拜托！面试请不要再问我SpringCloud底层原理","date":"2019-05-21T02:50:09.009Z","updated":"2019-05-30T00:52:25.422Z","comments":true,"pin":null,"path":"api/articles/拜托！面试请不要再问我SpringCloud底层原理.json","excerpt":"","keywords":null,"cover":"https://user-gold-cdn.xitu.io/2018/11/7/166ebffb48c481a3?imageView2/0/w/1280/h/960/ignore-error/1","content":"<blockquote>\n<p>本文转载自 <a href=\"https://juejin.im/post/5be13b83f265da6116393fc7\" target=\"_blank\" rel=\"noopener\">https://juejin.im/post/5be13b83f265da6116393fc7</a><br>欢迎关注微信公众号：<strong>石杉的架构笔记（id：shishan100）</strong></p>\n</blockquote>\n<p>每周一至五早8点半，精品技术文章准时送上！</p>\n<h2 id=\"目录\"><a href=\"#目录\" class=\"headerlink\" title=\"目录\"></a>目录</h2><p><strong>一、业务场景介绍</strong></p>\n<p><strong>二、Spring Cloud核心组件：Eureka</strong></p>\n<p><strong>三、Spring Cloud核心组件：Feign</strong></p>\n<p><strong>四、Spring Cloud核心组件：Ribbon</strong></p>\n<p><strong>五、Spring Cloud核心组件：Hystrix</strong></p>\n<p><strong>六、Spring Cloud核心组件：Zuul</strong></p>\n<p><strong>七、总结</strong></p>\n<hr>\n<h2 id=\"概述\"><a href=\"#概述\" class=\"headerlink\" title=\"概述\"></a>概述</h2><p>毫无疑问，Spring Cloud是目前微服务架构领域的翘楚，无数的书籍博客都在讲解这个技术。不过大多数讲解还停留在对Spring Cloud功能使用的层面，其底层的很多原理，很多人可能并不知晓。<strong>因此本文将通过大量的手绘图</strong>，给大家谈谈Spring Cloud微服务架构的底层原理。</p>\n<p>实际上，Spring Cloud是一个全家桶式的技术栈，包含了很多组件。本文先从其最核心的几个组件入手，来剖析一下其底层的工作原理。<strong>也就是Eureka、Ribbon、Feign、Hystrix、Zuul这几个组件。</strong></p>\n<hr>\n<h2 id=\"一、业务场景介绍\"><a href=\"#一、业务场景介绍\" class=\"headerlink\" title=\"一、业务场景介绍\"></a>一、业务场景介绍</h2><p>先来给大家说一个业务场景，假设咱们现在开发一个电商网站，要实现支付订单的功能，流程如下：</p>\n<pre><code>* 创建一个订单后，如果用户立刻支付了这个订单，我们需要将订单状态更新为“已支付”\n* 扣减相应的商品库存\n* 通知仓储中心，进行发货\n* 给用户的这次购物增加相应的积分</code></pre><p>针对上述流程，<strong>我们需要有订单服务、库存服务、仓储服务、积分服务</strong>。整个流程的大体思路如下：</p>\n<pre><code>* 用户针对一个订单完成支付之后，就会去找订单服务，更新订单状态\n* 订单服务调用库存服务，完成相应功能\n* 订单服务调用仓储服务，完成相应功能\n* 订单服务调用积分服务，完成相应功能</code></pre><p><strong>至此，整个支付订单的业务流程结束</strong></p>\n<hr>\n<p>下图这张图，清晰表明了各服务间的调用过程：<br><img src=\"https://user-gold-cdn.xitu.io/2018/11/7/166ebffb48c481a3?imageView2/0/w/1280/h/960/ignore-error/1\" alt></p>\n<p>好！有了业务场景之后，咱们就一起来看看Spring Cloud微服务架构中，这几个组件如何相互协作，各自发挥的作用以及其背后的原理。</p>\n<h2 id=\"二、Spring-Cloud核心组件：Eureka\"><a href=\"#二、Spring-Cloud核心组件：Eureka\" class=\"headerlink\" title=\"二、Spring Cloud核心组件：Eureka\"></a>二、Spring Cloud核心组件：Eureka</h2><p><strong>咱们来考虑第一个问题</strong>：订单服务想要调用库存服务、仓储服务，或者积分服务，怎么调用？</p>\n<pre><code>* 订单服务压根儿就不知道人家库存服务在哪台机器上啊！他就算想要发起一个请求，都不知道发送给谁，有心无力！\n* 这时候，就轮到Spring Cloud Eureka出场了。Eureka是微服务架构中的注册中心，专门负责服务的注册与发现。</code></pre><p>咱们来看看下面的这张图，结合图来仔细剖析一下整个流程：<br><img src=\"https://user-gold-cdn.xitu.io/2018/11/7/166ebffcb7ce31b8?imageView2/0/w/1280/h/960/ignore-error/1\" alt></p>\n<p>如上图所示，库存服务、仓储服务、积分服务中都有一个<strong>Eureka Client组件，这个组件专门负责将这个服务的信息注册到Eureka Server中</strong>。说白了，就是告诉Eureka Server，自己在哪台机器上，监听着哪个端口。而<strong>Eureka Server是一个注册中心，里面有一个注册表，保存了各服务所在的机器和端口号</strong></p>\n<p>订单服务里也有一个Eureka Client组件，这个Eureka Client组件会找Eureka Server问一下：库存服务在哪台机器啊？监听着哪个端口啊？仓储服务呢？积分服务呢？然后就可以把这些相关信息从Eureka Server的注册表中拉取到自己本地缓存起来。</p>\n<p>这时如果订单服务想要调用库存服务，不就可以找自己本地的Eureka Client问一下库存服务在哪台机器？监听哪个端口吗？收到响应后，紧接着就可以发送一个请求过去，调用库存服务扣减库存的那个接口！同理，如果订单服务要调用仓储服务、积分服务，也是如法炮制。</p>\n<p>总结一下：</p>\n<pre><code>* **Eureka Client：**负责将这个服务的信息注册到Eureka Server中\n* **Eureka Server：**注册中心，里面有一个注册表，保存了各个服务所在的机器和端口号</code></pre><h2 id=\"三、Spring-Cloud核心组件：Feign\"><a href=\"#三、Spring-Cloud核心组件：Feign\" class=\"headerlink\" title=\"三、Spring Cloud核心组件：Feign\"></a>三、Spring Cloud核心组件：Feign</h2><p>现在订单服务确实知道库存服务、积分服务、仓库服务在哪里了，同时也监听着哪些端口号了。<strong>但是新问题又来了：难道订单服务要自己写一大堆代码，跟其他服务建立网络连接，然后构造一个复杂的请求，接着发送请求过去，最后对返回的响应结果再写一大堆代码来处理吗？</strong></p>\n<hr>\n<p>这是上述流程翻译的代码片段，咱们一起来看看，体会一下这种绝望而无助的感受！！！</p>\n<p><strong>友情提示，前方高能：</strong><br><img src=\"https://user-gold-cdn.xitu.io/2018/11/7/166ebced960f2024?imageView2/0/w/1280/h/960/ignore-error/1\" alt><br><a href></a></p>\n<p><a href></a></p>\n<p>看完上面那一大段代码，有没有感到后背发凉、一身冷汗？实际上你进行服务间调用时，如果每次都手写代码，代码量比上面那段要多至少几倍，所以这个事压根儿就不是地球人能干的。</p>\n<p>既然如此，那怎么办呢？别急，Feign早已为我们提供好了优雅的解决方案。来看看如果用Feign的话，你的订单服务调用库存服务的代码会变成啥样？<br><img src=\"https://user-gold-cdn.xitu.io/2018/11/7/166ebcf01b773dd4?imageView2/0/w/1280/h/960/ignore-error/1\" alt><br><a href></a></p>\n<p><a href></a></p>\n<p>看完上面的代码什么感觉？是不是感觉整个世界都干净了，又找到了活下去的勇气！没有底层的建立连接、构造请求、解析响应的代码，直接就是用注解定义一个 FeignClient接口，然后调用那个接口就可以了。人家Feign Client会在底层根据你的注解，跟你指定的服务建立连接、构造请求、发起靕求、获取响应、解析响应，等等。这一系列脏活累活，人家Feign全给你干了。</p>\n<p>那么问题来了，Feign是如何做到这么神奇的呢？很简单，<strong>Feign的一个关键机制就是使用了动态代理</strong>。咱们一起来看看下面的图，结合图来分析：</p>\n<pre><code>* 首先，如果你对某个接口定义了@FeignClient注解，Feign就会针对这个接口创建一个动态代理\n* 接着你要是调用那个接口，本质就是会调用 Feign创建的动态代理，这是核心中的核心\n* Feign的动态代理会根据你在接口上的@RequestMapping等注解，来动态构造出你要请求的服务的地址\n* 最后针对这个地址，发起请求、解析响应</code></pre><p><img src=\"https://user-gold-cdn.xitu.io/2018/11/7/166ebfff505b2a20?imageView2/0/w/1280/h/960/ignore-error/1\" alt><br><a href></a></p>\n<p><a href></a></p>\n<h2 id=\"四、Spring-Cloud核心组件：Ribbon\"><a href=\"#四、Spring-Cloud核心组件：Ribbon\" class=\"headerlink\" title=\"四、Spring Cloud核心组件：Ribbon\"></a>四、Spring Cloud核心组件：Ribbon</h2><p>说完了Feign，还没完。现在新的问题又来了，如果人家库存服务部署在了5台机器上，如下所示：</p>\n<pre><code>* 192.168.169:9000\n* 192.168.170:9000\n* 192.168.171:9000\n* 192.168.172:9000\n* 192.168.173:9000</code></pre><p><strong>这下麻烦了！人家Feign怎么知道该请求哪台机器呢？</strong></p>\n<pre><code>* 这时Spring Cloud Ribbon就派上用场了。Ribbon就是专门解决这个问题的。它的作用是**负载均衡**，会帮你在每次请求时选择一台机器，均匀的把请求分发到各个机器上\n* Ribbon的负载均衡默认使用的最经典的**Round Robin轮询算法**。这是啥？简单来说，就是如果订单服务对库存服务发起10次请求，那就先让你请求第1台机器、然后是第2台机器、第3台机器、第4台机器、第5台机器，接着再来—个循环，第1台机器、第2台机器。。。以此类推。</code></pre><p><strong>此外，Ribbon是和Feign以及Eureka紧密协作，完成工作的，具体如下：</strong></p>\n<pre><code>* 首先Ribbon会从 Eureka Client里获取到对应的服务注册表，也就知道了所有的服务都部署在了哪些机器上，在监听哪些端口号。\n* 然后Ribbon就可以使用默认的Round Robin算法，从中选择一台机器\n* Feign就会针对这台机器，构造并发起请求。</code></pre><p>对上述整个过程，再来一张图，帮助大家更深刻的理解：<br><img src=\"https://user-gold-cdn.xitu.io/2018/11/7/166ec001dc155e98?imageView2/0/w/1280/h/960/ignore-error/1\" alt><br><a href></a></p>\n<p><a href></a></p>\n<hr>\n<h2 id=\"五、Spring-Cloud核心组件：Hystrix\"><a href=\"#五、Spring-Cloud核心组件：Hystrix\" class=\"headerlink\" title=\"五、Spring Cloud核心组件：Hystrix\"></a>五、Spring Cloud核心组件：Hystrix</h2><p>在微服务架构里，一个系统会有很多的服务。<strong>以本文的业务场景为例</strong>：订单服务在一个业务流程里需要调用三个服务。现在假设订单服务自己最多只有100个线程可以处理请求，然后呢，积分服务不幸的挂了，每次订单服务调用积分服务的时候，都会卡住几秒钟，然后抛出—个超时异常。</p>\n<p><strong>咱们一起来分析一下，这样会导致什么问题？</strong></p>\n<pre><code>1. 如果系统处于高并发的场景下，大量请求涌过来的时候，订单服务的100个线程都会卡在请求积分服务这块。导致订单服务没有一个线程可以处理请求\n1. 然后就会导致别人请求订单服务的时候，发现订单服务也挂了，不响应任何请求了</code></pre><p>上面这个，就是<strong>微服务架构中恐怖的服务雪崩问题</strong>，如下图所示：<br><img src=\"https://user-gold-cdn.xitu.io/2018/11/7/166ec0033f64a0a7?imageView2/0/w/1280/h/960/ignore-error/1\" alt></p>\n<p>如上图，这么多服务互相调用，要是不做任何保护的话，某一个服务挂了，就会引起连锁反应，导致别的服务也挂。比如积分服务挂了，会导致订单服务的线程全部卡在请求积分服务这里，没有一个线程可以工作，瞬间导致订单服务也挂了，别人请求订单服务全部会卡住，无法响应。</p>\n<p><strong>但是我们思考一下，就算积分服务挂了，订单服务也可以不用挂啊！为什么？</strong></p>\n<pre><code>* 我们结合业务来看：支付订单的时候，只要把库存扣减了，然后通知仓库发货就OK了\n* 如果积分服务挂了，大不了等他恢复之后，慢慢人肉手工恢复数据！为啥一定要因为一个积分服务挂了，就直接导致订单服务也挂了呢？不可以接受！</code></pre><p><strong>现在问题分析完了，如何解决？</strong></p>\n<p>这时就轮到Hystrix闪亮登场了。Hystrix是隔离、熔断以及降级的一个框架。啥意思呢？说白了，<strong>Hystrix会搞很多个小小的线程池</strong>，比如订单服务请求库存服务是一个线程池，请求仓储服务是一个线程池，请求积分服务是一个线程池。每个线程池里的线程就仅仅用于请求那个服务。</p>\n<p><strong>打个比方：现在很不幸，积分服务挂了，会咋样？</strong></p>\n<p>当然会导致订单服务里那个用来调用积分服务的线程都卡死不能工作了啊！但由于订单服务调用库存服务、仓储服务的这两个线程池都是正常工作的，所以这两个服务不会受到任何影响。</p>\n<p>这个时候如果别人请求订单服务，订单服务还是可以正常调用库存服务扣减库存，调用仓储服务通知发货。只不过调用积分服务的时候，每次都会报错。<strong>但是如果积分服务都挂了，每次调用都要去卡住几秒钟干啥呢？有意义吗？当然没有！</strong>所以我们直接对积分服务熔断不就得了，比如在5分钟内请求积分服务直接就返回了，不要去走网络请求卡住几秒钟，<strong>这个过程，就是所谓的熔断！</strong></p>\n<hr>\n<p><strong>那人家又说，兄弟，积分服务挂了你就熔断，好歹你干点儿什么啊！别啥都不干就直接返回啊？</strong>没问题，咱们就来个降级：每次调用积分服务，你就在数据库里记录一条消息，说给某某用户增加了多少积分，因为积分服务挂了，导致没增加成功！这样等积分服务恢复了，你可以根据这些记录手工加一下积分。<strong>这个过程，就是所谓的降级。</strong></p>\n<hr>\n<p>为帮助大家更直观的理解，接下来用一张图，梳理一下Hystrix隔离、熔断和降级的全流程：<br><img src=\"https://user-gold-cdn.xitu.io/2018/11/7/166ec004edf94426?imageView2/0/w/1280/h/960/ignore-error/1\" alt><br><a href></a></p>\n<p><a href></a></p>\n<h2 id=\"六、Spring-Cloud核心组件：Zuul\"><a href=\"#六、Spring-Cloud核心组件：Zuul\" class=\"headerlink\" title=\"六、Spring Cloud核心组件：Zuul\"></a>六、Spring Cloud核心组件：Zuul</h2><p>说完了Hystrix，接着给大家说说最后一个组件：Zuul，也就是微服务网关。<strong>这个组件是负责网络路由的。</strong>不懂网络路由？行，那我给你说说，如果没有Zuul的日常工作会怎样？</p>\n<p>假设你后台部署了几百个服务，现在有个前端兄弟，人家请求是直接从浏览器那儿发过来的。<strong>打个比方</strong>：人家要请求一下库存服务，你难道还让人家记着这服务的名字叫做inventory-service？部署在5台机器上？就算人家肯记住这一个，你后台可有几百个服务的名称和地址呢？难不成人家请求一个，就得记住一个？你要这样玩儿，那真是友谊的小船，说翻就翻！</p>\n<p>上面这种情况，压根儿是不现实的。所以一般微服务架构中都必然会设计一个网关在里面，像android、ios、pc前端、微信小程序、H5等等，不用去关心后端有几百个服务，就知道有一个网关，所有请求都往网关走，网关会根据请求中的一些特征，将请求转发给后端的各个服务。</p>\n<p>而且有一个网关之后，还有很多好处，比如可以做<strong>统一的降级、限流、认证授权、安全</strong>，等等。</p>\n<h2 id=\"七、总结：\"><a href=\"#七、总结：\" class=\"headerlink\" title=\"七、总结：\"></a>七、总结：</h2><p>最后再来总结一下，上述几个Spring Cloud核心组件，在微服务架构中，分别扮演的角色：</p>\n<pre><code>* **Eureka**：各个服务启动时，Eureka Client都会将服务注册到Eureka Server，并且Eureka Client还可以反过来从Eureka Server拉取注册表，从而知道其他服务在哪里\n* **Ribbon**：服务间发起请求的时候，基于Ribbon做负载均衡，从一个服务的多台机器中选择一台\n* **Feign**：基于Feign的动态代理机制，根据注解和选择的机器，拼接请求URL地址，发起请求\n* **Hystrix**：发起请求是通过Hystrix的线程池来走的，不同的服务走不同的线程池，实现了不同服务调用的隔离，避免了服务雪崩的问题\n* **Zuul**：如果前端、移动端要调用后端系统，统一从Zuul网关进入，由Zuul网关转发请求给对应的服务</code></pre><p>以上就是我们通过一个电商业务场景，阐述了Spring Cloud微服务架构几个核心组件的底层原理。</p>\n<p><strong>文字总结还不够直观？没问题！</strong>我们将Spring Cloud的5个核心组件通过一张图串联起来，再来直观的感受一下其底层的架构原理：<br><img src=\"https://user-gold-cdn.xitu.io/2018/11/7/166ec006b1536f43?imageView2/0/w/1280/h/960/ignore-error/1\" alt><br><a href></a></p>\n<p><a href></a></p>\n<p>如有收获，请帮忙转发，您的鼓励是作者最大的动力，谢谢！</p>\n<p><img src=\"https://user-gold-cdn.xitu.io/2018/11/6/166e9511e7162c8b?imageView2/0/w/1280/h/960/ignore-error/1\" alt></p>\n<p>Spring Cloud原创系列文章，将会持续更新</p>\n<p>欢迎关注微信公众号：<strong>石杉的架构笔记（id:shishan100）</strong></p>\n<p>十余年<strong>BAT架构经验</strong>倾囊相授</p>\n","text":"本文转载自 https://juejin.im/post/5be13b83f265da6116393fc7<br>欢迎关注微信公众号：石杉的架构笔记（id：shishan100）每周一至五早8点半，精品技术文章准时送上！目录一、业务场景介绍二、Spring Cloud核心组件：E","link":"","raw":null,"photos":[],"categories":[],"tags":[{"name":"Spring","slug":"Spring","count":2,"path":"api/tags/Spring.json"},{"name":"架构","slug":"架构","count":4,"path":"api/tags/架构.json"},{"name":"微服务","slug":"微服务","count":1,"path":"api/tags/微服务.json"},{"name":"服务器","slug":"服务器","count":1,"path":"api/tags/服务器.json"}]},{"title":"用大白话告诉你小白都能看懂的Hadoop架构原理","slug":"用大白话告诉你小白都能看懂的Hadoop架构原理","date":"2019-05-21T01:57:19.019Z","updated":"2019-05-30T00:52:25.433Z","comments":true,"pin":null,"path":"api/articles/用大白话告诉你小白都能看懂的Hadoop架构原理.json","excerpt":"","keywords":null,"cover":"https://user-gold-cdn.xitu.io/2018/11/13/1670dbfd11e62805?imageView2/0/w/1280/h/960/ignore-error/1","content":"<blockquote>\n<p>本文转载自 <a href=\"https://juejin.im/post/5beaf02ce51d457e90196069\" target=\"_blank\" rel=\"noopener\">https://juejin.im/post/5beaf02ce51d457e90196069</a> </p>\n</blockquote>\n<p><strong>欢迎关注个人微信号：石杉的架构笔记（id：shishan100）</strong></p>\n<p><strong>周一至周五早8点半！精品技术文章准时送上！</strong></p>\n<hr>\n<p><strong>往期文章</strong></p>\n<p>1、<a href=\"https://juejin.im/post/5be13b83f265da6116393fc7\" target=\"_blank\" rel=\"noopener\">拜托！面试请不要再问我Spring Cloud底层原理</a></p>\n<p>2、<a href=\"https://juejin.im/post/5be3f8dcf265da613a5382ca\" target=\"_blank\" rel=\"noopener\">【双11狂欢的背后】微服务注册中心如何承载大型系统的千万级访问？</a></p>\n<p>3、<a href=\"https://juejin.im/post/5be83e166fb9a049a7115580\" target=\"_blank\" rel=\"noopener\">【性能优化之道】每秒上万并发下的Spring Cloud参数优化实战</a></p>\n<p>4、<a href=\"https://juejin.im/post/5be99a68e51d4511a8090440\" target=\"_blank\" rel=\"noopener\">微服务架构如何保障双11狂欢下的99.99%高可用</a></p>\n<hr>\n<h3 id=\"目录\"><a href=\"#目录\" class=\"headerlink\" title=\"目录\"></a>目录</h3><p>一、前奏</p>\n<p>二、HDFS的NameNode架构原理</p>\n<h1 id=\"一、前奏\"><a href=\"#一、前奏\" class=\"headerlink\" title=\"一、前奏\"></a>一、前奏</h1><p>Hadoop是目前大数据领域最主流的一套技术体系，包含了多种技术。</p>\n<p>包括HDFS（分布式文件系统），YARN（分布式资源调度系统），MapReduce（分布式计算系统），等等。</p>\n<p>有些朋友可能听说过Hadoop，但是却不太清楚他到底是个什么东西，这篇文章就用大白话给各位阐述一下。</p>\n<p>假如你现在公司里的数据都是放在MySQL里的，那么就全部放在一台数据库服务器上，我们就假设这台服务器的磁盘空间有2T吧，<strong>大家先看下面这张图。</strong></p>\n<p><img src=\"https://user-gold-cdn.xitu.io/2018/11/13/1670dbfd11e62805?imageView2/0/w/1280/h/960/ignore-error/1\" alt><br><a href></a></p>\n<p><a href></a></p>\n<p>现在问题来了，你不停的往这台服务器的MySQL里放数据，结果数据量越来越大了，超过了2T的大小了，现在咋办？</p>\n<p>你说，我可以搞多台MySQL数据库服务器，分库分表啊！每台服务器放一部分数据不就得了。<strong>如上图所示！</strong></p>\n<p>好，没问题，那咱们搞3台数据库服务器，3个MySQL实例，然后每台服务器都可以2T的数据。</p>\n<p>现在我问你一个问题，<strong>所谓的大数据是在干什么？</strong></p>\n<p>我们来说一下大数据最初级的一个使用场景。假设你有一个电商网站，现在要把这个电商网站里所有的用户在页面和APP上的点击、购买、浏览的行为日志都存放起来分析。</p>\n<p>你现在把这些数据全都放在了3台MySQL服务器，数据量很大，但还是勉强可以放的下。</p>\n<p>某天早上，你的boss来了。要看一张报表，比如要看每天网站的X指标、Y指标、Z指标，等等，二三十个数据指标。</p>\n<p>好了，兄弟，现在你尝试去从那些点击、购买、浏览的日志里，通过写一个SQL来分析出那二三十个指标试试看？</p>\n<p>我跟你打赌，你绝对会写出来一个几百行起步，甚至上千行的超级复杂大SQL。这个SQL，你觉得他能运行在分库分表后的3台MySQL服务器上么？</p>\n<p>如果你觉得可以的话，那你一定是不太了解MySQL分库分表后有多坑，几百行的大SQL跨库join，各种复杂的计算，根本不现实。</p>\n<p>所以说，大数据的存储和计算压根儿不是靠MySQL来搞的，因此，Hadoop、Spark等大数据技术体系才应运而生。</p>\n<p>本质上，Hadoop、Spark等大数据技术，其实就是一系列的分布式系统。</p>\n<p>比如hadoop中的HDFS，就是大数据技术体系中的核心基石，<strong>负责分布式存储数据，这是啥意思？别急，继续往下看。</strong></p>\n<p>HDFS全称是Hadoop Distributed File System，是Hadoop的分布式文件系统。</p>\n<p>它由很多机器组成，每台机器上运行一个DataNode进程，负责管理一部分数据。</p>\n<p>然后有一台机器上运行了NameNode进程，NameNode大致可以认为是负责管理整个HDFS集群的这么一个进程，他里面存储了HDFS集群的所有元数据。</p>\n<p>然后有很多台机器，每台机器存储一部分数据！好，HDFS现在可以很好的存储和管理大量的数据了。</p>\n<p>这时候你肯定会有疑问：MySQL服务器也不是这样的吗？你要是这样想，那就大错特错了。</p>\n<p>这个事情不是你想的那么简单的，HDFS天然就是分布式的技术，所以你上传大量数据，存储数据，管理数据，天然就可以用HDFS来做。</p>\n<p>如果你硬要基于MySQL分库分表这个事儿，会痛苦很多倍，因为MySQL并不是设计为分布式系统架构的，他在分布式数据存储这块缺乏很多数据保障的机制。</p>\n<p>好，你现在用HDFS分布式存储了数据，接着不就是要分布式来计算这些数据了吗？</p>\n<p>对于分布式计算：</p>\n<pre><code>* 很多公司用Hive写几百行的大SQL（底层基于MapReduce）\n* 也有很多公司开始慢慢的用Spark写几百行的大SQL（底层是Spark Core引擎）。</code></pre><p>总之就是写一个大SQL，人家会拆分为很多的计算任务，放到各个机器上去，每个计算任务就负责计算一小部分数据，这就是所谓的分布式计算。</p>\n<p>这个，绝对比你针对分库分表的MySQL来跑几百行大SQL要靠谱的多。</p>\n<p>对于上述所说，老规矩，同样给大家来一张图，大伙儿跟着图来仔细捋一下整个过程。</p>\n<p><img src=\"https://user-gold-cdn.xitu.io/2018/11/13/1670dc005dc982dc?imageView2/0/w/1280/h/960/ignore-error/1\" alt><br><a href></a></p>\n<p><a href></a></p>\n<h1 id=\"二、HDFS的NameNode架构原理\"><a href=\"#二、HDFS的NameNode架构原理\" class=\"headerlink\" title=\"二、HDFS的NameNode架构原理\"></a>二、HDFS的NameNode架构原理</h1><p>好了，前奏铺垫完之后，进入正题。本文其实主要就是讨论一下HDFS集群中的NameNode的核心架构原理。</p>\n<p>NameNode有一个很核心的功能：<strong>管理整个HDFS集群的元数据</strong>，比如说文件目录树、权限的设置、副本数的设置，等等。</p>\n<p>下面就用最典型的文件目录树的维护，来给大家举例说明，<strong>我们看看下面的图。</strong>现在有一个客户端系统要上传一个1TB的大文件到HDFS集群里。</p>\n<p><img src=\"https://user-gold-cdn.xitu.io/2018/11/13/1670dc022eeddf7f?imageView2/0/w/1280/h/960/ignore-error/1\" alt><br><a href></a></p>\n<p><a href></a></p>\n<p>此时他会先跟NameNode通信，说：大哥，我想创建一个新的文件，他的名字叫“/usr/hive/warehouse/access_20180101.log”，大小是1TB，你看行不？</p>\n<p>然后NameNode就会在自己内存的文件目录树里，在指定的目录下搞一个新的文件对象，名字就是“access_20180101.log”。</p>\n<p>这个文件目录树不就是HDFS非常核心的一块元数据，维护了HDFS这个分布式文件系统中，有哪些目录，有哪些文件，对不对？</p>\n<p>但是有个问题，这个文件目录树是在NameNode的内存里的啊！</p>\n<p>这可坑爹了，你把重要的元数据都放在内存里，<strong>万一NameNode不小心宕机了可咋整？元数据不就全部丢失了？</strong></p>\n<p>可你要是每次都频繁的修改磁盘文件里的元数据，性能肯定是极低的啊！毕竟这是大量的磁盘随机读写！</p>\n<p>没关系，<strong>我们来看看HDFS优雅的解决方案。</strong></p>\n<p>每次内存里改完了，写一条edits log，元数据修改的操作日志到磁盘文件里，不修改磁盘文件内容，就是顺序追加，这个性能就高多了。</p>\n<p>每次NameNode重启的时候，把edits log里的操作日志读到内存里回放一下，不就可以恢复元数据了？</p>\n<p><strong>大家顺着上面的文字，把整个过程，用下面这张图跟着走一遍。</strong></p>\n<hr>\n<p><img src=\"https://user-gold-cdn.xitu.io/2018/11/13/1670dc03f2942281?imageView2/0/w/1280/h/960/ignore-error/1\" alt><br><a href></a></p>\n<p><a href></a></p>\n<p>但是问题又来了，那edits log如果越来越大的话，岂不是每次重启都会很慢？因为要读取大量的edits log回放恢复元数据！</p>\n<p>所以HDFS说，我可以这样子啊，我引入一个新的磁盘文件叫做<strong>fsimage</strong>，然后呢，再引入一个<strong>JournalNodes</strong>集群，以及一个<strong>Standby NameNode</strong>（备节点）。</p>\n<p>每次Active NameNode（主节点）修改一次元数据都会生成一条edits log，<strong>除了写入本地磁盘文件，还会写入JournalNodes集群。</strong></p>\n<p>然后Standby NameNode就可以从JournalNodes集群拉取edits log，应用到自己内存的文件目录树里，跟Active NameNode保持一致。</p>\n<p>然后每隔一段时间，Standby NameNode都把自己内存里的文件目录树写一份到磁盘上的fsimage，这可不是日志，这是完整的一份元数据。<strong>这个操作就是所谓的checkpoint检查点操作。</strong></p>\n<p>然后把这个fsimage上传到到Active NameNode，接着清空掉Active NameNode的旧的edits log文件，这里可能都有100万行修改日志了！</p>\n<p>然后Active NameNode继续接收修改元数据的请求，再写入edits log，写了一小会儿，这里可能就几十行修改日志而已！</p>\n<p>如果说此时，Active NameNode重启了，bingo！没关系，只要把Standby NameNode传过来的fsimage直接读到内存里，<strong>这个fsimage直接就是元数据</strong>，不需要做任何额外操作，纯读取，效率很高！</p>\n<p>然后把新的edits log里少量的几十行的修改日志回放到内存里就ok了！</p>\n<p>这个过程的启动速度就快的多了！因为不需要回放大量上百万行的edits log来恢复元数据了！如下图所示。</p>\n<p><img src=\"https://user-gold-cdn.xitu.io/2018/11/13/1670dc062e2fa9e8?imageView2/0/w/1280/h/960/ignore-error/1\" alt><br><a href></a></p>\n<p><a href></a></p>\n<p>此外，<strong>大家看看上面这张图</strong>，现在咱们有俩NameNode。</p>\n<pre><code>* 一个是主节点对外提供服务接收请求\n* 另外一个纯就是接收和同步主节点的edits log以及执行定期checkpoint的备节点。</code></pre><p>大家有没有发现！他们俩内存里的元数据几乎是一模一样的啊！</p>\n<p>所以呢，如果Active NameNode挂了，是不是可以立马切换成Standby NameNode对外提供服务？</p>\n<p><strong>这不就是所谓的NameNode主备高可用故障转移机制么！</strong></p>\n<p>接下来大家再想想，HDFS客户端在NameNode内存里的文件目录树，新加了一个文件。</p>\n<p>但是这个时候，人家要把数据上传到多台DataNode机器上去啊，<strong>这可是一个1TB的大文件！咋传呢？</strong></p>\n<p>很简单，<strong>把1TB的大文件拆成N个block</strong>，每个block是128MB。1TB = 1024GB = 1048576MB，一个block是128MB，那么就是对应着8192个block。</p>\n<p>这些block会分布在不同的机器上管理着，比如说一共有100台机器组成的集群，那么每台机器上放80个左右的block就ok了。</p>\n<p>但是问题又来了，那如果这个时候1台机器宕机了，不就导致80个block丢失了？</p>\n<p>也就是说上传上去的1TB的大文件，会丢失一小部分数据啊。没关系！HDFS都考虑好了！</p>\n<p>它会<strong>默认给每个block搞3个副本</strong>，一模一样的副本，分放在不同的机器上，如果一台机器宕机了，同一个block还有另外两个副本在其他机器上呢！</p>\n<p><strong>大伙儿看看下面这张图</strong>。每个block都在不同的机器上有3个副本，任何一台机器宕机都没事！还可以从其他的机器上拿到那个block。</p>\n<p>这下子，你往HDFS上传一个1TB的大文件，可以高枕无忧了吧！</p>\n<p><img src=\"https://user-gold-cdn.xitu.io/2018/11/13/1670dc07b1a880a6?imageView2/0/w/1280/h/960/ignore-error/1\" alt><br><a href></a></p>\n<p><a href></a></p>\n<p>OK，上面就是大白话加上一系列手绘图，给大家先聊聊小白都能听懂的Hadoop的基本架构原理</p>\n<p>接下来会给大家聊聊HDFS，这个作为世界上最优秀的分布式存储系统，承载高并发请求、高性能文件上传的一些核心机制以及原理。</p>\n<p><strong>《大规模集群下Hadoop如何承载每秒上千次的高并发访问》，</strong>敬请期待</p>\n<p><strong>《【冰山下的秘密】Hadoop如何将TB级大文件的上传性能提升上百倍？》</strong>，敬请期待</p>\n<p><strong>如有收获，请帮忙转发，您的鼓励是作者最大的动力，谢谢！</strong></p>\n<p><strong>一大波微服务、分布式、高并发、高可用的**</strong>原创系列<strong>**文章正在路上,</strong></p>\n<hr>\n<p><strong>**欢迎扫描下方二维码</strong>，持续关注：**\n<img src=\"https://user-gold-cdn.xitu.io/2018/11/12/167088310d1d57b1?imageView2/0/w/1280/h/960/ignore-error/1\" alt></p>\n<p><strong>石杉的架构笔记（id:shishan100）</strong></p>\n<p><strong>十余年BAT架构经验倾囊相授</strong></p>\n","text":"本文转载自 https://juejin.im/post/5beaf02ce51d457e90196069 欢迎关注个人微信号：石杉的架构笔记（id：shishan100）周一至周五早8点半！精品技术文章准时送上！往期文章1、拜托！面试请不要再问我Spring Cloud底层原理","link":"","raw":null,"photos":[],"categories":[],"tags":[{"name":"MySQL","slug":"MySQL","count":2,"path":"api/tags/MySQL.json"},{"name":"架构","slug":"架构","count":4,"path":"api/tags/架构.json"},{"name":"HDFS","slug":"HDFS","count":1,"path":"api/tags/HDFS.json"},{"name":"Hadoop","slug":"Hadoop","count":1,"path":"api/tags/Hadoop.json"}]},{"title":"高并发架构的CDN知识介绍","slug":"高并发架构的CDN知识介绍","date":"2019-04-29T16:00:00.000Z","updated":"2019-05-30T00:52:25.436Z","comments":true,"pin":null,"path":"api/articles/高并发架构的CDN知识介绍.json","excerpt":"<p>对一次网络请求过程的了解程度，一是展现你的专业知识；二是深刻的理解，让你在大型网站架构中做出更适合、可靠的架构。而DNS是这一切的出发点，本文结合一张常用架构图，来描述一下这个过程。</p>","keywords":null,"cover":"https://user-gold-cdn.xitu.io/2019/4/29/16a676b8dec824bc?imageView2/0/w/1280/h/960/ignore-error/1","content":null,"text":"对一次网络请求过程的了解程度，一是展现你的专业知识；二是深刻的理解，让你在大型网站架构中做出更适合、可靠的架构。而DNS是这一切的出发点，本文结合一张常用架构图，来描述一下这个过程。  本文来自于 https://juejin.im/post/5cc681fc6fb9a0320e","link":"","raw":null,"photos":[],"categories":[],"tags":[{"name":"架构","slug":"架构","count":4,"path":"api/tags/架构.json"}]},{"title":"高并发架构的TCP知识介绍","slug":"高并发架构的TCP知识介绍","date":"2019-05-07T06:21:38.038Z","updated":"2019-05-30T00:52:25.436Z","comments":true,"pin":null,"path":"api/articles/高并发架构的TCP知识介绍.json","excerpt":"","keywords":null,"cover":"https://user-gold-cdn.xitu.io/2019/5/7/16a90774db961af1?imageView2/0/w/1280/h/960/ignore-error/1","content":"<blockquote>\n<p>本文转载自 <a href=\"https://juejin.im/post/5cd103e2f265da03804383e1\" target=\"_blank\" rel=\"noopener\">https://juejin.im/post/5cd103e2f265da03804383e1</a> </p>\n</blockquote>\n<p>做为一个有追求的程序员，不能只满足增删改查，我们要对系统全方面无死角掌控。掌握了这些基本的网络知识后，相信一方面日常排错中会事半功倍，另一方面日常架构中不得不考虑的高并发问题，理解了这些底层协议也是会如虎添翼。</p>\n<p>本文不会单纯给大家讲讲TCP三次握手、四次挥手就完事了。如果只是哪样的话，我直接贴几个连接就完事了。我希望把实际工作中的很多点能够串起来讲给大家。当然为了文章完整，我依然会从 <strong>三次握手</strong> 起头。</p>\n<h2 id=\"再说TCP状态变更过程\"><a href=\"#再说TCP状态变更过程\" class=\"headerlink\" title=\"再说TCP状态变更过程\"></a>再说TCP状态变更过程</h2><p>不管是三次握手、还是四次挥手，他们都是完成了TCP不同状态的切换。进而影响各种数据的传输情况。下面从三次握手开始分析。<br>本文图片有部分来自网络，若有侵权，告知即焚</p>\n<h3 id=\"三次握手\"><a href=\"#三次握手\" class=\"headerlink\" title=\"三次握手\"></a>三次握手</h3><p>来看看三次握手的图，估计大家看这图都快看吐了，不过为什么每次面试、回忆的时候还是想不起呢？我再来抄抄这锅剩饭吧！<br><img src=\"https://user-gold-cdn.xitu.io/2019/5/7/16a90774db961af1?imageView2/0/w/1280/h/960/ignore-error/1\" alt=\"tcp-1st\"></p>\n<p>首先当服务端处于 <strong>listen</strong> 状态的时候，我们就可以再客户端发起监听了，此时客户端会处于 <strong>SYN_SENT</strong> 状态。服务端收到这个消息会返回一个 <strong>SYN</strong> 并且同时 <strong>ACK</strong> 客户端的请求，之后服务端便处于 <strong>SYN_RCVD</strong> 状态。这个时候客户端收到了服务端的 <strong>SYN&amp;ACK</strong>，就会发送对服务端的 <strong>ACK</strong>，之后便处于 <strong>ESTABLISHED</strong> 状态。服务端收到了对自己的 <strong>ACK</strong> 后也会处于 <strong>ESTABLISHED</strong> 状态。</p>\n<p>经常在面试中可能有人提问：为什么握手要3次，不是2次或者4次呢？</p>\n<p>首先说4次握手，其实为了保证可靠性，这个握手次数可以一直循环下去；但是这没有一个终止就没有意义了。所以3次，保证了各方消息有来有回就足够了。当然这里可能有一种情况是，客户端发送的 <strong>ACK</strong> 在网络中被丢了。那怎么办？</p>\n<pre><code>1. 其实大部分时候，我们连接建立完成就会立刻发送数据，所以如果服务端没有收到 **ACK** 没关系，当收到数据就会认为连接已经建立；\n1. 如果连接建立后不立马传输数据，那么服务端认为连接没有建立成功会周期性重发 **SYN&amp;ACK** 直到客户端确认成功。</code></pre><p>再说为什么2次握手不行呢？2次握手我们可以想象是没有三次握手最后的 <strong>ACK</strong>, 在实际中确实会出现客户端发送 <strong>ACK</strong> 服务端没有收到的情况（上面的情况一），那么这是否说明两次握手也是可行的呢？ 看下情况二，2次握手当服务端发送消息后，就认为建立成功，而恰巧此时又没有数据传输。这就会带来一种资源浪费的情况。比如：客户端可能由于延时发送了多个连接情况，当服务端每收到一个请求回复后就认为连接建立成功，但是这其中很多求情都是延时产生的重复连接，浪费了很多宝贵的资源。</p>\n<p>因此综上所述，从资源节省、效率3次握手都是最合适的。话又回来三次握手的真实意义其实就是协商传输数据用的：<strong>序列号与窗口大小</strong>。</p>\n<p>下面我们通过抓包再来看一下真实的情况是否如上所述。</p>\n<div class=\"highlight-wrap\"autocomplete=\"off\" autocorrect=\"off\" autocapitalize=\"off\" spellcheck=\"false\" contenteditable=\"true\"data-rel=\"PLAIN\"><figure class=\"iseeu highlight plain\"><table><tr><td class=\"gutter\"><pre><span class=\"line\">1</span><br><span class=\"line\">2</span><br><span class=\"line\">3</span><br></pre></td><td class=\"code\"><pre><span class=\"line\">20:33:26.583598 IP 192.168.0.102.58165 &gt; 103.235.46.39.80: Flags [S], seq 621839080, win 65535, options [mss 1460,nop,wscale 6,nop,nop,TS val 1050275400 ecr 0,sackOK,eol], length 0</span><br><span class=\"line\">20:33:26.660754 IP 103.235.46.39.80 &gt; 192.168.0.102.58165: Flags [S.], seq 1754967387, ack 621839081, win 8192, options [mss 1452,nop,wscale 5,nop,nop,nop,nop,nop,nop,nop,nop,nop,nop,nop,nop,sackOK,eol], length 0</span><br><span class=\"line\">20:33:26.660819 IP 192.168.0.102.58165 &gt; 103.235.46.39.80: Flags [.], ack 1754967388, win 4096, length 0</span><br></pre></td></tr></table></figure></div>\n\n<p><strong>抓包：</strong> <code>sudo tcpdump -n host www.baidu.com -S</code></p>\n<pre><code>* `S` 表示 SYN\n* `.` 表示 ACK\n* `P` 表示 传输数据\n* `F` 表示 FIN</code></pre><h3 id=\"四次挥手\"><a href=\"#四次挥手\" class=\"headerlink\" title=\"四次挥手\"></a>四次挥手</h3><p>挥手，就是说数据传完了，同志们再见！</p>\n<p><img src=\"https://user-gold-cdn.xitu.io/2019/5/7/16a90774dcd5d27e?imageView2/0/w/1280/h/960/ignore-error/1\" alt=\"tcp-3th\"></p>\n<p>这里有个问题需要注意下，其实客户端、服务端都能够主动发起关闭操作，谁调用 <code>close()</code> 就先发送关闭的请求。当然一般的流程，发起建立连接的一方会主动发起关闭请求（http中）。</p>\n<p>关于4次挥手的过程，我就不多解释了，这里有两个重要的状态我需要解释下，这都是我亲自经历过的线上故障，<strong>close_wait</strong> 与 <strong>time_wait</strong>。</p>\n<p>先给大家一个命令，统计tcp的各种状态情况。下面表格内容就来自这个命令的统计。<br>netstat -n | awk ‘/^tcp/ {++S[$NF]} END {for(a in S) print a, S[a]}’</p>\n<table>\n<thead>\n<tr>\n<th align=\"center\">Tcp状态</th>\n<th align=\"center\">连接数</th>\n</tr>\n</thead>\n<tbody><tr>\n<td align=\"center\">CLOSE_WAIT</td>\n<td align=\"center\">505</td>\n</tr>\n<tr>\n<td align=\"center\">ESTABLISHED</td>\n<td align=\"center\">808</td>\n</tr>\n<tr>\n<td align=\"center\">TIME_WAIT</td>\n<td align=\"center\">3481</td>\n</tr>\n<tr>\n<td align=\"center\">SYN_SENT</td>\n<td align=\"center\">1</td>\n</tr>\n<tr>\n<td align=\"center\">SYN_RECV</td>\n<td align=\"center\">1</td>\n</tr>\n<tr>\n<td align=\"center\">LAST_ACK</td>\n<td align=\"center\">2</td>\n</tr>\n<tr>\n<td align=\"center\">FIN_WAIT2</td>\n<td align=\"center\">2</td>\n</tr>\n<tr>\n<td align=\"center\">FIN_WAIT1</td>\n<td align=\"center\">1</td>\n</tr>\n</tbody></table>\n<p><strong>大量的CLOSE_WAIT</strong> 这个在我之前的一篇文章 <a href=\"https://link.juejin.im?target=https%3A%2F%2Fdayutalk.cn%2F2018%2F12%2F08%2F%25E7%25BA%25BF%25E4%25B8%258A%25E5%25A4%25A7%25E9%2587%258FCLOSE_WAIT%25E5%2588%2586%25E6%259E%2590%2F\" target=\"_blank\" rel=\"noopener\">线上大量CLOSE_WAIT原因分析</a> 已经有过介绍，它会导致大量的socket无法释放。而每个socket都是一个文件，是会占用资源的。这个问题主要是代码问题。它出现在被动关闭的一方（习惯称为server）。</p>\n<p><strong>大量的TIME_WAIT</strong> 这个问题在日常中经常看到，流量一高就出现大量的该情况。该状态出现在主动发起关闭的一方。该状态一般等待的时间设为 2MSL后自动关闭，<strong>MSL是Maximum Segment Lifetime，报文最大生存时间</strong>，如果报文超过这个时间，就会被丢弃。处于该状态下的socket也是不能被回收使用的。线上我就遇到这种情况，每次大流量的时候，每台机器处于该状态的socket就多达10w+，远远比处于 <code>Established</code> 状态的socket多的多，导致很多时候服务响应能力下降。这个一方面可以通过调整内核参数处理，另一方面避免使用太多的短链接，可以采用连接池来提升性能。另外在代码层面可能是由于某些地方没有关闭连接导致的，也需要检查业务代码。</p>\n<p>上面两个状态一定要牢记发生在哪一方，这方便我们快速定位问题。</p>\n<p>最后这里还是放上挥手时的抓包数据：</p>\n<div class=\"highlight-wrap\"autocomplete=\"off\" autocorrect=\"off\" autocapitalize=\"off\" spellcheck=\"false\" contenteditable=\"true\"data-rel=\"PLAIN\"><figure class=\"iseeu highlight plain\"><table><tr><td class=\"gutter\"><pre><span class=\"line\">1</span><br><span class=\"line\">2</span><br><span class=\"line\">3</span><br><span class=\"line\">4</span><br></pre></td><td class=\"code\"><pre><span class=\"line\">20:33:26.750607 IP 192.168.0.102.58165 &gt; 103.235.46.39.80: Flags [F.], seq 621839159, ack 1754967720, win 4096, length 0</span><br><span class=\"line\">20:33:26.827472 IP 103.235.46.39.80 &gt; 192.168.0.102.58165: Flags [.], ack 621839160, win 776, length 0</span><br><span class=\"line\">20:33:26.827677 IP 103.235.46.39.80 &gt; 192.168.0.102.58165: Flags [F.], seq 1754967720, ack 621839160, win 776, length 0</span><br><span class=\"line\">20:33:26.827729 IP 192.168.0.102.58165 &gt; 103.235.46.39.80: Flags [.], ack 1754967721, win 4096, length 0</span><br></pre></td></tr></table></figure></div>\n\n<p>不多不少，刚好4次。</p>\n<h3 id=\"TCP状态变更\"><a href=\"#TCP状态变更\" class=\"headerlink\" title=\"TCP状态变更\"></a>TCP状态变更</h3><p>网络上有一张TCP状态机的图，我觉得太复杂了，用自己的方式搞个简单点的容易理解的。我从两个角度来说明状态的变更。</p>\n<pre><code>* 一个是客户端\n* 一个是服务端</code></pre><p>看下面两张图的时候，请一定结合上面三次握手、四次挥手的时序图一起看，加深理解。</p>\n<h3 id=\"客户端状态变更\"><a href=\"#客户端状态变更\" class=\"headerlink\" title=\"客户端状态变更\"></a>客户端状态变更</h3><p><img src=\"https://user-gold-cdn.xitu.io/2019/5/7/16a90774ec12ff59?imageView2/0/w/1280/h/960/ignore-error/1\" alt=\"tcp-4th\"></p>\n<p>通过这张图，大家是否能够清晰明了的知道 TCP 在客户端上的变更情况了呢？</p>\n<h3 id=\"服务端状态变更\"><a href=\"#服务端状态变更\" class=\"headerlink\" title=\"服务端状态变更\"></a>服务端状态变更</h3><p><img src=\"https://user-gold-cdn.xitu.io/2019/5/7/16a90774eb5d5b79?imageView2/0/w/1280/h/960/ignore-error/1\" alt=\"tcp-5th\"></p>\n<p>这一张图描述了 TCP 状态在服务端的变迁。</p>\n<h2 id=\"TCP的流量控制与拥塞控制\"><a href=\"#TCP的流量控制与拥塞控制\" class=\"headerlink\" title=\"TCP的流量控制与拥塞控制\"></a>TCP的流量控制与拥塞控制</h2><p>我们常说TCP是面向连接的，UDP是无连接的。那么TCP这个面向连接主要解决的是什么问题呢？</p>\n<p>这里继续把三次握手的抓包数据贴出来分析下：</p>\n<div class=\"highlight-wrap\"autocomplete=\"off\" autocorrect=\"off\" autocapitalize=\"off\" spellcheck=\"false\" contenteditable=\"true\"data-rel=\"PLAIN\"><figure class=\"iseeu highlight plain\"><table><tr><td class=\"gutter\"><pre><span class=\"line\">1</span><br><span class=\"line\">2</span><br><span class=\"line\">3</span><br></pre></td><td class=\"code\"><pre><span class=\"line\">20:33:26.583598 IP 192.168.0.102.58165 &gt; 103.235.46.39.80: Flags [S], seq 621839080, win 65535, options [mss 1460,nop,wscale 6,nop,nop,TS val 1050275400 ecr 0,sackOK,eol], length 0</span><br><span class=\"line\">20:33:26.660754 IP 103.235.46.39.80 &gt; 192.168.0.102.58165: Flags [S.], seq 1754967387, ack 621839081, win 8192, options [mss 1452,nop,wscale 5,nop,nop,nop,nop,nop,nop,nop,nop,nop,nop,nop,nop,sackOK,eol], length 0</span><br><span class=\"line\">20:33:26.660819 IP 192.168.0.102.58165 &gt; 103.235.46.39.80: Flags [.], ack 1754967388, win 4096, length 0</span><br></pre></td></tr></table></figure></div>\n\n<p>上面我们说到 <code>TCP</code> 的三次握手最重要的就是协商传输数据用的序列号。那这个序列号究竟有些什么用呢？这个序号能够帮助后续两端进行确认数据包是否收到，解决顺序、丢包问题；另外我们还可以看到有一个 <strong>win</strong> 字段，这是双方交流的窗口大小，这在每次传输数据过程中也会携带。主要是告诉对方，我窗口是这么大，别发多了或者别发太少。</p>\n<p>总结下，TCP的几个特点是：</p>\n<pre><code>* 顺序问题，依靠序号\n* 丢包问题，依靠序号\n* 流量控制，依靠滑动窗口\n* 拥塞控制，依靠拥塞窗口+滑动窗口\n* 连接维护，三次握手/四次挥手</code></pre><h3 id=\"顺序与丢包问题\"><a href=\"#顺序与丢包问题\" class=\"headerlink\" title=\"顺序与丢包问题\"></a>顺序与丢包问题</h3><p>这个问题其实应该很好理解。由于数据在传输前我们已经有序号了，这里注意一下这个序号是随机的，重复的概率极地，避免了程序发生乱入的可能性。</p>\n<p>由于我们每个数据包有序号，虽然发送与到达可能不是顺序的，但是TCP层收到数据后，可以根据序号进行重新排列；另外在这个排列过程中，发现有了1，2，3，5，6这几个包，一检查就知道4要么延时未到达，要么丢包了，等待重传。</p>\n<p>这里需要重要说明的一点是。为了提升效率，TCP其实并不是收到一个包就发一个ack。那是如何ACK的呢？还是以上面为例，TCP收到了1,2,3,5,6这几个包，它可能会发送一个 <code>ack ，seq=3</code> 的确认包，这样次一次确认了3个包。但是它不会发送 5,6 的ack。因为4没有收到啊！一旦4延时到达或者重发到达，就会发送一个 <code>ack, seq=6</code>，又一次确认了3个包。</p>\n<h3 id=\"流量控制与拥塞控制\"><a href=\"#流量控制与拥塞控制\" class=\"headerlink\" title=\"流量控制与拥塞控制\"></a>流量控制与拥塞控制</h3><p>这两个概念说实话，让我理解了挺长时间，主要是对它们各自控制的内容以及相互之间是否有作用一直没有闹清楚。</p>\n<p>先大概说下：</p>\n<pre><code>* 流量控制：是根据接收方的窗口大小来感知我这次能够传多少数据给对方；———— 滑动窗口\n* 拥塞控制：而拥塞控制主要是避免网络拥塞，它考虑的问题更多。根据综合因素来觉得发多少数据给对方；———— 滑动窗口&amp;拥塞窗口</code></pre><p>举个例子说下，比如：A给B发送数据，通过握手后，A知道B一次可以收1000的数据（B有这么大的处理能力），那么这个时候滑动窗口就可以设置成1000。那是不是最后真的可以一次发这么多数据给B呢？还不是，这时候得问问拥塞窗口，老兄，现在网络情况怎么样？一次运1000的数据有压力吗？拥塞窗口一通计算说不行，现在是高峰期，最多只能有600的货上路。最终这次传数据的时候就是 600 的标注。大家也可以关注抓包数据的 <strong>win</strong> 值，一直在动态调整。</p>\n<p>当然另外一种情况是滑动窗口比拥塞窗口小，虽然运输能力强，但是接收能力有限，这时候就要取滑动窗口的值来实际发生。所以它们二者之间是有关系的。</p>\n<p>所以具体到每次能够发送多少数据，有这么一个公式：<br>LastByteSend - LastByteAcked &lt;= min{cwnd,rwnd}</p>\n<pre><code>* LastByteSend 是最后一个发送的字节的序号\n* LastByteAcked 最后一个被确认的字节的序号</code></pre><p>这两个相减得到的是本次能够发送的数据，这个数据一定小于或等于 cwnd 与 rwnd 中最小的一个值。相信大家能够理清楚。</p>\n<p>那么这部分知识对于实际工作中有什么作用呢？指导意义就是：如果你的业务很重要、很核心一定不要混布；二是如果你的服务忽快忽慢，而确信依赖服务没有问题，检查下机器对应的网络情况；三是窗口这个速度控制机制，在我们进行服务设计的时候，非常具有参考意义。是不是有点消息队列的感觉？（很多消息队列都是匀速的，我们是否可以加一个窗口的概念来进行优化呢？）</p>\n<h2 id=\"是什么限制了你的连接\"><a href=\"#是什么限制了你的连接\" class=\"headerlink\" title=\"是什么限制了你的连接\"></a>是什么限制了你的连接</h2><p>到了最关键的地方了，精华我都是留到最后讲。下面放一张网上找的socket操作步骤图，画的太好了我就直接用了。<br><img src=\"https://user-gold-cdn.xitu.io/2019/5/7/16a90775097ada31?imageView2/0/w/1280/h/960/ignore-error/1\" alt=\"tcp-6th\"></p>\n<p>我们假设我的服务端就是 <code>Nginx</code> ，我来尝试解读一下。当客户端调用 <code>connect()</code> 时候就会发起三次握手，这次握手的时候有几个元素唯一确定了这次通信（或者说这个socket），<strong>[源IP:源Port， 目的IP:目的Port]</strong> ，当然这个socket还不是最终用来传输数据的socket，一旦握手完成后，服务端会在返回一个 <strong>socket</strong> 专门用来后续的数据传输。这里暂且把第一个socket叫 <strong>监听socket</strong>，第二个叫 <strong>传输socket</strong> 方便后文叙述。</p>\n<p>为什么要这么设计呢？大家想一想，如果监听的socket还要负责数据的收发，请问这个服务端的效率如何提升？什么东西、谁都往这个socket里边丢，太复杂！</p>\n<h3 id=\"提高连接常用套路\"><a href=\"#提高连接常用套路\" class=\"headerlink\" title=\"提高连接常用套路\"></a>提高连接常用套路</h3><p>到了这一步，我们现在先停下来算算自己的服务器机器能够有多少连接呢？这个极限又是如何一步步被突破呢？</p>\n<p>先说 <strong>监听socket</strong> ，服务器的prot一般都是固定的，服务器的ip当然也是固定的（单机）。那么上面的结构 <strong>[源IP:源Port， 目的IP:目的Port]</strong> 其实只有客户端的ip与端口可以发生变化。假设客户端用的是IPv4，那么理论连接数是：2^32(ip数) /* 2^16(端口数) = 2^48。</p>\n<p>看起来这个值蛮大的。但是真的能够有这么多连接吗？不可能的，因为每一个socket都需要消耗内存；以及每一个进程的文件描述符是有上限的。这些都限制了最终的连接数。</p>\n<p>那么如何进行调和呢？我知道的操作有：多进程、多线程、IO多路服用、协程等手段组合使用。</p>\n<h3 id=\"多进程\"><a href=\"#多进程\" class=\"headerlink\" title=\"多进程\"></a>多进程</h3><p>也就是监听是一个进程，一旦accept后，对于 <strong>传输socket</strong> 我们就fork一个新的子进程来处理。但是这种方式太重，fork一个进程、销毁一个进程都是特别费事的。单机对进程的创建上限也是有限制的。</p>\n<h3 id=\"多线程\"><a href=\"#多线程\" class=\"headerlink\" title=\"多线程\"></a>多线程</h3><p>线程比进程要轻量级的多，它会共享父进程的很多资源，比如：文件描述符、进程空间，它就是多了一个引用。因此它的创建、销毁更加容易。每一个 <strong>传输socket</strong> 在这里就交给了线程来处理。</p>\n<p>但是不管是多进程、还是多线程都存在一个问题，一个连接对应一个进程或者协程。这都很难逃脱 <strong>C10K</strong> 的问题。那么该怎么办呢？</p>\n<h3 id=\"IO多路复用\"><a href=\"#IO多路复用\" class=\"headerlink\" title=\"IO多路复用\"></a>IO多路复用</h3><p>IO多路复用是什么意思呢？在上面单纯的多进程、多线程模型中，一个进程或线程只能处理一个连接。用了IO多路复用后，我一个进程或线程就能处理多个连接。</p>\n<p>我们都知道 <code>Nginx</code> 非常高效，它的结构是：master + worker，worker 会在 80、443端口上来监听请求。它的worker一般设置为 cpu 的cores数，那么这么少的子进程是如何解决超多连接的呢？这里其实每个worker就采用了 epoll 模型（当然IO多路复用还有个select，这里就不说了）。</p>\n<p>处于监听状态的worker，会把所有 <strong>监听socket</strong> 加入到自己的epoll中。当这些socket都在epoll中时，如果某个socket有事件发生就会立即被回调唤醒（这涉及epoll的红黑树，讲不清楚不细说了）。这种模式，大大增加了每个进程可以管理的socket数量，上限直接可以上升到进程能够操作的最大文件描述符。</p>\n<p>一般机器可以设置百万级别文件描述符，所以单机单进程就是百万连接，<strong>epoll是解决C10K的利器，很多开源软件用到了它。</strong><br>这里说下，并不是所有的worker都是同时处于监听端口的状态，这涉及到nginx惊群、抢自旋锁的问题，不再本文范围内不多说。</p>\n<h3 id=\"关于ulimit\"><a href=\"#关于ulimit\" class=\"headerlink\" title=\"关于ulimit\"></a>关于ulimit</h3><p>在文章的最后，补充一些单机文件描述符设置的问题。我们常说连接数受限于文件描述符，这是为什么？</p>\n<p>因为在linux上一切皆文件，故每一个socket都是被当作一个文件看待，那么每个文件就会有一个文件描述符。在linux中每一个进程中都有一个数组保存了该进程需要的所有文件描述符。这个文件描述符其实就是这个数组的 <code>key</code> ，它的 <code>value</code> 是一个指针，指向的就是打开的对应文件。</p>\n<p>关于文件描述符有两点注意：</p>\n<pre><code>1. 它对应的其实是一个linux上的文件\n1. 文件描述符本身这个值在不同进程中是可以重复的</code></pre><p>另外补充一点，单机设置的ulimit的上线受限与系统的两个配置：<br>fs.nr_open，进程级别</p>\n<p>fs.file-max，系统级别</p>\n<p>fs.nr_open 总是应该小于等于 fs.file-max，这两个值的设置也不是随意可以操作，因为设置的越大，系统资源消耗越多，所以需要根据真实情况来进行设置。</p>\n<hr>\n<p>至此，本篇长文就完结了。这跟上篇 <a href=\"https://link.juejin.im?target=https%3A%2F%2Fdayutalk.cn%2F2019%2F04%2F25%2F%25E4%25BB%258E%25E5%25B8%25B8%25E8%25A7%2581%25E6%259E%25B6%25E6%259E%2584%25E8%25B0%2588CDN%25E4%25B8%258EDNS%2F\" target=\"_blank\" rel=\"noopener\">高并发架构的CDN知识介绍</a> 属于一个系列，高并发架构需要理解的网络基础知识。</p>\n<p>后面还会写一下 HTTP/HTTPS 的知识。然后关于高并发网络相关的东西就算完结。我会开启下一个篇章。</p>\n<hr>\n<p>如果你想对网络协议了解更多，推荐一个课程：<br><img src=\"https://user-gold-cdn.xitu.io/2019/5/7/16a90774dd3eebbf?imageView2/0/w/1280/h/960/ignore-error/1\" alt=\"tcp-7th\"></p>\n","text":"本文转载自 https://juejin.im/post/5cd103e2f265da03804383e1 做为一个有追求的程序员，不能只满足增删改查，我们要对系统全方面无死角掌控。掌握了这些基本的网络知识后，相信一方面日常排错中会事半功倍，另一方面日常架构中不得不考虑的高并发问","link":"","raw":null,"photos":[],"categories":[],"tags":[{"name":"架构","slug":"架构","count":4,"path":"api/tags/架构.json"},{"name":"TCP/IP","slug":"TCP-IP","count":1,"path":"api/tags/TCP-IP.json"}]}]}